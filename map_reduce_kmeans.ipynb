{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import random\nimport sklearn\nfrom operator import add\nimport math\nimport pyspark\nimport pandas as pd\nimport numpy as np\nfrom pyspark.sql import SparkSession\nfrom sklearn.preprocessing import MinMaxScaler"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "fe6c47cd-ccec-4b62-adee-16d5749fda44"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "iris = \"/FileStore/tables/iris.csv\"\nglass = \"/FileStore/tables/glass.csv\"\nparkinsons = \"/FileStore/tables/parkinsons.csv\"\nfile_type = \"csv\"\niris = spark.read.csv(iris,header=True,inferSchema=True)\npark = spark.read.csv(parkinsons,header=True,inferSchema=True)\nglass = spark.read.csv(glass,header=True,inferSchema=True)\n"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "6482be4c-f067-47c9-b0ac-35c938b94601"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "def preprocess(df):\n    df=df.toPandas()\n    scale_cols=df.columns[:-1] #choosing columns to perform min max on\n    df_class=[]\n    class_col=df.columns[-1:] #choosing class column\n    t=np.array(df[class_col]) #saving class values as a list\n    for i in range(len(t)):\n        df_class.append(t[i][0])\n    scaler=MinMaxScaler() #performing min max standartization\n    scaler.fit(df[scale_cols])\n    df = pd.DataFrame(scaler.transform(df[scale_cols]),columns=df[scale_cols].columns)\n    df = spark.createDataFrame(df)\n    return df,df_class #returning standartized df and classes list\n    "
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "4d13b686-5d0e-4754-aa93-937128381449"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "# a func that the map will apply on the data that receives the dataframe and the a list of random centroids\ndef f_for_map(p1,centroids):\n    dmin=math.inf #searching for min dist, therefore initializing to inf\n    idx = -1 #initializing of minimal index to -1\n    for j in range(len(centroids)): #iterating through the centroids list\n        d = np.linalg.norm(p1-centroids[j]) #calculating distance of the current row to the current centroid\n        if(d<dmin): #saving minimal dist index and value\n            dmin=d\n            idx = j\n    return(idx,p1)#returns a tapple with index of centroid and the assigned row"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "848bbb9b-ac84-427b-8eb2-84c6dc0d0707"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "def avg_reduce(k,c_dict): \n    return (k[0],k[1]/c_dict[k[0]]) # a reduce function to calculate the average of the "
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "8f9f8db7-bd06-4029-9590-ea7be4a181e4"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "#the function that performs k-means algorithm as requested with default vals\ndef kmeans(df,k,ct=0.0001,i=30,exp=10):\n    res=preprocess(df)#preprocessing function\n    class_list=res[1] #output of processing\n    ch_list=[]\n    ari_list=[]\n    for w in range(exp): #experiment iteration\n        dist_sum=math.inf #searching to be less then threshhold - ct , so initializing to inf\n        for j in range(i): #within experiment iterations\n            if j==0: # at first iteration the k initial centroids are random\n                centroids = res[0].rdd.takeSample(False,k)\n            kv_list=res[0].rdd.map(lambda row:f_for_map(np.array(row),centroids)) #applying map on the dataframe with f_for_map func that assigns rows to centroids\n            count_dict=kv_list.countByKey() #calculating number of df rows assigned to each centroid\n            kv_list=kv_list.map(lambda row:(row[0],np.array(row[1]))) #processing data to be in a shape that fits a reduce func\n            preds=kv_list.map(lambda row:row[0]) #extracting predicted values per row\n            red_kv_list=kv_list.reduceByKey(lambda a,b:a+b) #reduce func that summs all the rows per key by thier dimensions\n            new_centroids=red_kv_list.map(lambda k:avg_reduce(k,count_dict)).collect() #a mapper that dives summs with the count value in order to calc average per key\n            coords=[]\n            for z in range(len(new_centroids)): #extracting dimensions of the new calculated centroids\n                coords.append(new_centroids[z][1])\n            s=0 #initialization of sum variable to sum the distances from old centroids to new ones\n            q=[] #initialization \n            for d in range(len(centroids)):\n                for f in range(len(centroids[d])):\n                    s+=abs(centroids[d][f]-coords[d][f])\n            dis_sum=s #updating condition sum\n            if(dis_sum<ct): #checking convergens condition\n                break\n        centroids=coords #updating centroids variable to be the new calculated centroids\n        ch=sklearn.metrics.calinski_harabasz_score(df.rdd.collect(),preds.collect()) #calculation of CH score for each experiment\n        ari=sklearn.metrics.adjusted_rand_score(class_list,preds.collect()) #calculation of ARI score for each experiment\n        ch_list.append(ch) #saving results to lists in order to calculate mean and std later\n        ari_list.append(ari) \n    print(\"The Mean and Standard Deviation of Calinski-Harabasz score for \" + str(k) + \" clusters are: \" + str((np.array(ch_list).mean(),np.array(ch_list).std())) + \"\\n\" + \"The Mean and Standard Deviation of Adjusted-Rand-Index score for \" + str(k) + \" clusters are: \" + str((np.array(ari_list).mean(),(np.array(ari_list).std()) )))\n  \n        "
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "9223e130-5215-415b-bb0b-189a173d8785"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "kmeans(park,5)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "6d34bbe9-2f76-49e9-9c86-837804782d2b"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "datasetInfos": [],
       "data": "The Mean and Standard Deviation of Calinski-Harabasz score for 5 clusters are: (24.432037994644947, 13.478368369354003)\nThe Mean and Standard Deviation of Adjusted-Rand-Index score for 5 clusters are: (0.1030855689294409, 0.04082675950860078)\n",
       "removedWidgets": [],
       "addedWidgets": {},
       "metadata": {},
       "type": "ansi",
       "arguments": {}
      }
     },
     "data": {
      "text/plain": [
       "The Mean and Standard Deviation of Calinski-Harabasz score for 5 clusters are: (24.432037994644947, 13.478368369354003)\nThe Mean and Standard Deviation of Adjusted-Rand-Index score for 5 clusters are: (0.1030855689294409, 0.04082675950860078)\n"
      ]
     }
    }
   ],
   "execution_count": 0
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "notebookName": "hw2_kmeans",
   "dashboards": [],
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "language": "python",
   "widgets": {},
   "notebookOrigID": 772311010669399
  },
  "kernelspec": {
   "name": "pycharm-ac7e203e",
   "language": "python",
   "display_name": "PyCharm (Desktop)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}